# Batch-ETL-Pipeline-for-Financial-Transactions-using-Airflow-Spark-and-Snowflake
Designed and implemented a scalable batch ETL pipeline for financial transaction data to detect fraudulent activities. Used Apache Airflow to orchestrate workflows that extract raw data from CSV files, transform it using Apache Spark for feature engineering and anomaly detection, and load it into Snowflake for advanced analytics. Scheduled DAGs for daily batch processing and integrated logging and monitoring. Focused on data cleansing, timestamp handling, fraud flagging, and aggregations. This end-to-end pipeline simulates real-world financial data operations and highlights strong understanding of workflow orchestration, distributed data processing, and cloud-based data warehousing.
